<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=theme-color><title>Reinforcement Learning and Recommender Systems &#183; Kenny F.</title>
<meta name=title content="Reinforcement Learning and Recommender Systems &#183; Kenny F."><meta name=description content="march & april, looking forward to summer"><meta name=keywords content="Markov,Reinforce-Learning,System-Recomendation,"><link rel=canonical href=https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/><link type=text/css rel=stylesheet href=/css/main.bundle.min.1f2f46b2353c1d40685837f6d6dc78af3e37f0947f857ee7bbfbf2185daf232b4e1ec92df3547b377d03a95318b266074dcf9dcd2f99afbbca81c178fc6f05b3.css integrity="sha512-Hy9GsjU8HUBoWDf21tx4rz438JR/hX7nu/vyGF2vIytOHskt81R7N30DqVMYsmYHTc+dzS+Zr7vKgcF4/G8Fsw=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.c624cb647ef05d7443c34168a02e9f8542d33be2f04b4cac0c87158a03a648df21b582451fcc4af0f7528134b9f2a8968e37f9fb408592b92ed9326e4e9211e3.js integrity="sha512-xiTLZH7wXXRDw0FooC6fhULTO+LwS0ysDIcVigOmSN8htYJFH8xK8PdSgTS58qiWjjf5+0CFkrku2TJuTpIR4w==" data-copy=Copy data-copied=Copied></script><script src=/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7+kfJ6kKCJxQGC+8wm+Bz9JucDjDTGNew=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/"><meta property="og:site_name" content="Kenny F."><meta property="og:title" content="Reinforcement Learning and Recommender Systems"><meta property="og:description" content="march & april, looking forward to summer"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-14T21:29:01+00:00"><meta property="article:modified_time" content="2023-01-14T21:29:01+00:00"><meta property="article:tag" content="Markov"><meta property="article:tag" content="Reinforce-Learning"><meta property="article:tag" content="System-Recomendation"><meta property="og:image" content="https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/featured-image.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/featured-image.jpg"><meta name=twitter:title content="Reinforcement Learning and Recommender Systems"><meta name=twitter:description content="march & april, looking forward to summer"><meta name=twitter:image content="https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/featured-image.jpg"><meta property="og:image" content="https://kennyfh.github.io/posts/2023-01-14-rl-recommendation-systems/featured-image.jpg"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Reinforcement Learning and Recommender Systems","headline":"Reinforcement Learning and Recommender Systems","description":"march \u0026 april, looking forward to summer","inLanguage":"en","url":"https:\/\/kennyfh.github.io\/posts\/2023-01-14-rl-recommendation-systems\/","author":{"@type":"Person","name":"Kenny Flores"},"copyrightYear":"2023","dateCreated":"2023-01-14T21:29:01\u002b00:00","datePublished":"2023-01-14T21:29:01\u002b00:00","dateModified":"2023-01-14T21:29:01\u002b00:00","keywords":["Markov","Reinforce-Learning","System-Recomendation"],"mainEntityOfPage":"true","wordCount":"3907"}]</script><meta name=author content="Kenny Flores"><link href=https://github.com/kennyfh rel=me><link href=https://gitlab.com/mrkenny rel=me><link href=https://linkedin.com/in/kennyfh rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.7e7e35e3ef02b7b437449a44ca3fac62ec1ed39cb8312b680a00fe8ac60badc95b063b694636b8440856f7f5e8c2cc9e6b0efb581179b2656c7e1e97558c7096.css integrity="sha512-fn414+8Ct7Q3RJpEyj+sYuwe05y4MStoCgD+isYLrclbBjtpRja4RAhW9/Xowsyeaw77WBF5smVsfh6XVYxwlg=="><script defer type=text/javascript src=/js/katex.bundle.735b6f1542c0acf03d492dcdf14710c2e277474a64ae8ce49ef56563e17d142f9d3f4466e82cf12679968f50985628e08948ec341fba2df746545d0fd337d81d.js integrity="sha512-c1tvFULArPA9SS3N8UcQwuJ3R0pkrozknvVlY+F9FC+dP0Rm6CzxJnmWj1CYVijgiUjsNB+6LfdGVF0P0zfYHQ==" id=katex-render></script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px] z-100"><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3 pt-[2px] pr-0 pb-[3px] pl-0"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Kenny F.</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=About>About</p></a><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Resume>Resume</p></a><a href=/publications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Publications>Publications</p></a><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Posts</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Projects>Projects</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="Search (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="Search (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><div id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=About>About</p></a></li><li class=mt-1><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Resume>Resume</p></a></li><li class=mt-1><a href=/publications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Publications>Publications</p></a></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Posts</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Projects>Projects</p></a></li></ul></div></div></div></div><script>(function(){var e=$(".main-menu"),t=window.location.pathname;e.find('a[href="'+t+'"]').each(function(e,t){$(t).children("p").addClass("active")})})()</script></div></div><script type=text/javascript src=/js/background-blur.min.78ad5090e7c0c2533bac9dd23bdb1f723f834e5c3626627bd31017cc139ce0651b529877862129cee49e380ab2fdcd8baef28ae3e086bc54957fdaa7758775b2.js integrity="sha512-eK1QkOfAwlM7rJ3SO9sfcj+DTlw2JmJ70xAXzBOc4GUbUph3hiEpzuSeOAqy/c2LrvKK4+CGvFSVf9qndYd1sg==" data-target-id=menu-blur></script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Reinforcement Learning and Recommender Systems</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2023-01-14T21:29:01+00:00>14 January 2023</time><span class="px-2 text-primary-500">&#183;</span><span>3907 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">19 mins</span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Kenny Flores" src=/img/prof_pic_hu_c9250efab31b1774.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Kenny Flores</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Artificial Intelligence Researcher @ University of Seville</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:kflores1@us.es target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/kennyfh target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://gitlab.com/mrkenny target=_blank aria-label=Gitlab rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M510.486 284.482l-27.262-83.963c.012.038.016.077.028.115-.013-.044-.021-.088-.033-.132v-.01L429.1 33.871a21.328 21.328.0 00-20.445-14.6A21.038 21.038.0 00388.466 34L337.094 192.154H175L123.533 33.989A21.033 21.033.0 00103.35 19.274h-.113A21.467 21.467.0 0082.86 34L28.888 200.475l-.008.021v0c-.013.042-.019.084-.033.127.012-.038.017-.077.029-.115L1.514 284.482a30.6 30.6.0 0011.117 34.283L248.893 490.427c.035.026.074.041.109.067.1.072.2.146.3.214-.1-.065-.187-.136-.282-.2.015.012.033.02.05.031s.027.015.041.024h.006a11.992 11.992.0 001.137.7c.054.03.1.068.157.1.033.016.064.038.1.054s.053.02.077.032.038.015.056.023c.044.021.092.034.136.057.205.1.421.178.633.264.2.082.389.177.592.248l.025.011c.034.012.064.028.1.04s.083.032.125.046l.05.012c.053.016.11.024.163.039.019.006.042.009.063.015.284.086.579.148.872.213.115.026.225.062.341.083.017.0.032.009.05.012.038.008.073.021.112.027.062.011.122.031.186.04.049.007.1.0.151.012h.033a11.918 11.918.0 001.7.136h.019a11.971 11.971.0 001.7-.136h.033c.05-.008.1.0.153-.012s.124-.029.187-.04c.038-.006.073-.019.11-.027.017.0.032-.009.049-.012.118-.023.231-.059.349-.084.288-.064.578-.126.861-.21.019-.006.039-.008.059-.014.055-.017.113-.024.169-.041.016-.006.035-.007.051-.012.044-.013.086-.032.129-.047s.063-.028.1-.041l.026-.01c.214-.076.417-.175.627-.261s.394-.154.584-.245c.047-.023.1-.036.142-.059.018-.009.04-.015.058-.024s.053-.02.078-.033.068-.04.1-.056c.056-.028.106-.069.161-.1a12.341 12.341.0 001.132-.695c.029-.02.062-.035.092-.056.008-.006.017-.009.024-.015.035-.026.076-.043.11-.068l236.3-171.666A30.6 30.6.0 00510.486 284.482zM408.8 49.48l46.342 142.674H362.46zm-305.6.0 46.428 142.675H56.948zM26.817 299.251a6.526 6.526.0 01-2.361-7.308l20.34-62.42L193.835 420.6zm38.245-82.972h92.411L223.354 419.22zm183.416 273.83c-.047-.038-.092-.079-.138-.118-.009-.008-.018-.018-.028-.026-.091-.075-.18-.152-.268-.231-.172-.15-.341-.3-.5-.462.014.012.029.022.043.035l.055.046a12.191 12.191.0 001.091.929l.012.011c.018.013.033.03.051.045C248.689 490.263 248.58 490.19 248.478 490.109zm7.514-48.482L217.226 322.21 182.839 216.279H329.253zm7.935 48.107c-.091.079-.178.157-.27.233l-.032.028c-.047.038-.091.079-.136.117-.1.08-.209.152-.313.229.018-.013.033-.032.053-.044l.009-.009a11.69 11.69.0 001.086-.926c.014-.013.03-.024.044-.036s.038-.03.054-.047C264.262 489.435 264.1 489.586 263.927 489.734zm90.7-273.455h92.4l-18.91 24.23-139.468 178.7zm130.567 82.967L318.2 420.563 467.284 229.538l20.258 62.393A6.528 6.528.0 01485.189 299.246z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/kennyfh target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#1-recommendation-systems>1. Recommendation Systems</a><ul><li><a href=#11-traditional-recommendation-systems>1.1 Traditional Recommendation Systems</a><ul><li><a href=#111-collaborative-filtering-based-recommendation-systems>1.1.1 Collaborative Filtering-Based Recommendation Systems</a></li><li><a href=#112-content-based-recommendation-systems>1.1.2 Content-Based Recommendation Systems</a></li></ul></li><li><a href=#12-evaluation-metrics-for-recommendation-algorithms>1.2 Evaluation Metrics for Recommendation Algorithms</a></li><li><a href=#13-limitations-of-supervised-learning>1.3 Limitations of Supervised Learning</a><ul><li><a href=#short-sighted-recommendation>Short-Sighted Recommendation</a></li><li><a href=#system-bias>System Bias</a></li></ul></li></ul></li><li><a href=#2-recommendation-systems-based-on-rl>2. Recommendation Systems Based on RL</a><ul><li><a href=#21-what-is-reinforcement-learning>2.1 What is Reinforcement Learning?</a></li><li><a href=#22-elements-of-reinforcement-learning>2.2 Elements of Reinforcement Learning</a><ul><li><a href=#policy>Policy</a></li><li><a href=#reward-signals>Reward Signals</a></li><li><a href=#value-functions>Value Functions</a></li></ul></li><li><a href=#23-why-use-rl-in-recommendation-algorithms>2.3 Why Use RL in Recommendation Algorithms?</a></li><li><a href=#24-rl-algorithms-in-recommendation-systems>2.4 RL Algorithms in Recommendation Systems</a><ul><li><a href=#241-deep-q-learning>2.4.1 Deep Q Learning</a></li><li><a href=#242-soft-actor-critic-sac>2.4.2 Soft Actor-Critic (SAC)</a></li><li><a href=#243-reinforce-top-k-off-policy-correction>2.4.3 REINFORCE Top-K Off-Policy Correction</a></li></ul></li><li><a href=#25-challenges-in-applying-reinforcement-learning-to-recommendation-systems>2.5 Challenges in Applying Reinforcement Learning to Recommendation Systems</a><ul><li><a href=#large-action-space>Large Action Space</a></li><li><a href=#costly-exploration>Costly Exploration</a></li><li><a href=#off-policy-learning>Off-Policy Learning</a></li><li><a href=#partial-observability>Partial Observability</a></li><li><a href=#noisy-rewards>Noisy Rewards</a></li></ul></li></ul></li><li><a href=#3-applications>3. Applications</a><ul><li><a href=#31-recnn-recommendation-toolkit>3.1 RecNN: Recommendation Toolkit</a><ul><li><a href=#311-features>3.1.1 Features</a></li><li><a href=#312-getting-started>3.1.2 Getting Started</a></li></ul></li><li><a href=#32-rl4rs>3.2 RL4RS</a><ul><li><a href=#321-features>3.2.1 Features</a></li><li><a href=#322-getting-started>3.2.2 Getting Started</a></li></ul></li></ul></li><li><a href=#4-practical-case-rl-for-youtube-video-recommendation-systems>4. Practical Case: RL for YouTube Video Recommendation Systems</a><ul><li><a href=#41-introduction>4.1 Introduction</a></li><li><a href=#42-candidate-generation>4.2 Candidate Generation</a></li><li><a href=#43-reinforcement-learning-in-recommendation-systems>4.3 Reinforcement Learning in Recommendation Systems</a></li><li><a href=#44-model-construction>4.4 Model Construction</a><ul><li><a href=#agent-and-reward>Agent and Reward</a></li><li><a href=#states>States</a></li><li><a href=#actions>Actions</a></li></ul></li><li><a href=#45-addressing-machine-learning-limitations>4.5 Addressing Machine Learning Limitations</a><ul><li><a href=#myopic-recommendation>Myopic Recommendation</a></li><li><a href=#system-bias-1>System Bias</a></li></ul></li></ul></li><li><a href=#5-conclusions>5. Conclusions</a></li><li><a href=#6-references>6. References</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#1-recommendation-systems>1. Recommendation Systems</a><ul><li><a href=#11-traditional-recommendation-systems>1.1 Traditional Recommendation Systems</a><ul><li><a href=#111-collaborative-filtering-based-recommendation-systems>1.1.1 Collaborative Filtering-Based Recommendation Systems</a></li><li><a href=#112-content-based-recommendation-systems>1.1.2 Content-Based Recommendation Systems</a></li></ul></li><li><a href=#12-evaluation-metrics-for-recommendation-algorithms>1.2 Evaluation Metrics for Recommendation Algorithms</a></li><li><a href=#13-limitations-of-supervised-learning>1.3 Limitations of Supervised Learning</a><ul><li><a href=#short-sighted-recommendation>Short-Sighted Recommendation</a></li><li><a href=#system-bias>System Bias</a></li></ul></li></ul></li><li><a href=#2-recommendation-systems-based-on-rl>2. Recommendation Systems Based on RL</a><ul><li><a href=#21-what-is-reinforcement-learning>2.1 What is Reinforcement Learning?</a></li><li><a href=#22-elements-of-reinforcement-learning>2.2 Elements of Reinforcement Learning</a><ul><li><a href=#policy>Policy</a></li><li><a href=#reward-signals>Reward Signals</a></li><li><a href=#value-functions>Value Functions</a></li></ul></li><li><a href=#23-why-use-rl-in-recommendation-algorithms>2.3 Why Use RL in Recommendation Algorithms?</a></li><li><a href=#24-rl-algorithms-in-recommendation-systems>2.4 RL Algorithms in Recommendation Systems</a><ul><li><a href=#241-deep-q-learning>2.4.1 Deep Q Learning</a></li><li><a href=#242-soft-actor-critic-sac>2.4.2 Soft Actor-Critic (SAC)</a></li><li><a href=#243-reinforce-top-k-off-policy-correction>2.4.3 REINFORCE Top-K Off-Policy Correction</a></li></ul></li><li><a href=#25-challenges-in-applying-reinforcement-learning-to-recommendation-systems>2.5 Challenges in Applying Reinforcement Learning to Recommendation Systems</a><ul><li><a href=#large-action-space>Large Action Space</a></li><li><a href=#costly-exploration>Costly Exploration</a></li><li><a href=#off-policy-learning>Off-Policy Learning</a></li><li><a href=#partial-observability>Partial Observability</a></li><li><a href=#noisy-rewards>Noisy Rewards</a></li></ul></li></ul></li><li><a href=#3-applications>3. Applications</a><ul><li><a href=#31-recnn-recommendation-toolkit>3.1 RecNN: Recommendation Toolkit</a><ul><li><a href=#311-features>3.1.1 Features</a></li><li><a href=#312-getting-started>3.1.2 Getting Started</a></li></ul></li><li><a href=#32-rl4rs>3.2 RL4RS</a><ul><li><a href=#321-features>3.2.1 Features</a></li><li><a href=#322-getting-started>3.2.2 Getting Started</a></li></ul></li></ul></li><li><a href=#4-practical-case-rl-for-youtube-video-recommendation-systems>4. Practical Case: RL for YouTube Video Recommendation Systems</a><ul><li><a href=#41-introduction>4.1 Introduction</a></li><li><a href=#42-candidate-generation>4.2 Candidate Generation</a></li><li><a href=#43-reinforcement-learning-in-recommendation-systems>4.3 Reinforcement Learning in Recommendation Systems</a></li><li><a href=#44-model-construction>4.4 Model Construction</a><ul><li><a href=#agent-and-reward>Agent and Reward</a></li><li><a href=#states>States</a></li><li><a href=#actions>Actions</a></li></ul></li><li><a href=#45-addressing-machine-learning-limitations>4.5 Addressing Machine Learning Limitations</a><ul><li><a href=#myopic-recommendation>Myopic Recommendation</a></li><li><a href=#system-bias-1>System Bias</a></li></ul></li></ul></li><li><a href=#5-conclusions>5. Conclusions</a></li><li><a href=#6-references>6. References</a></li></ul></nav></div></details><script>(function(){"use strict";const n=.33,s="#TableOfContents",o=".anchor",i='a[href^="#"]',a="li ul",r="active";function c(e,t){const s=window.scrollY+window.innerHeight*t,o=[...document.querySelectorAll('#TableOfContents a[href^="#"]')],n=new Set(o.map(e=>e.getAttribute("href").substring(1)));for(let t=e.length-1;t>=0;t--){const o=e[t].getBoundingClientRect().top+window.scrollY;if(o<=s&&n.has(e[t].id))return e[t].id}return e.find(e=>n.has(e.id))?.id||""}function e({toc:e,anchors:t,links:n,scrollOffset:s,collapseInactive:o}){const i=c(t,s);if(!i)return;if(n.forEach(e=>{const t=e.getAttribute("href")===`#${i}`;if(e.classList.toggle(r,t),o){const n=e.closest("li")?.querySelector("ul");n&&(n.style.display=t?"":"none")}}),o){const n=e.querySelector(`a[href="#${CSS.escape(i)}"]`);let t=n;for(;t&&t!==e;)t.tagName==="UL"&&(t.style.display=""),t.tagName==="LI"&&t.querySelector("ul")?.style.setProperty("display",""),t=t.parentElement}}function t(){const t=document.querySelector(s);if(!t)return;const c=!1,l=[...document.querySelectorAll(o)],d=[...t.querySelectorAll(i)];c&&t.querySelectorAll(a).forEach(e=>e.style.display="none");const r={toc:t,anchors:l,links:d,scrollOffset:n,collapseInactive:c};window.addEventListener("scroll",()=>e(r),{passive:!0}),window.addEventListener("hashchange",()=>e(r),{passive:!0}),e(r)}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",t):t()})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>Reinforcement learning has seen significant advancements in robotics and video games, but with the growing importance of generating personalised content for users on online platforms like Netflix or Spotify, building recommendation systems based on reinforcement learning could offer a series of positive improvements compared to traditional supervised learning systems.</p><p>In this study, we will discuss the different types of recommendation systems, the challenges we will face when transitioning from traditional systems to the use of reinforcement learning, a practical case where it is being used in the real world, and an overview of current trends and developments in the field.</p><h2 class="relative group">1. Recommendation Systems<div id=1-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#1-recommendation-systems aria-label=Anchor>#</a></span></h2><p><strong>Recommendation systems</strong> (often called &ldquo;recommender systems&rdquo;) are algorithms that aim to <strong>suggest relevant content or products</strong> to users based on their preferences and other relevant information.</p><p>These systems are essential across various industries (such as e-commerce or multimedia entertainment applications) to help users discover content that might interest them, increasing the likelihood that they will remain on the platform and generate revenue for the company.</p><h3 class="relative group">1.1 Traditional Recommendation Systems<div id=11-traditional-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#11-traditional-recommendation-systems aria-label=Anchor>#</a></span></h3><p>Within recommendation systems, there are various techniques that provide valuable and personalised recommendations to each user. The following figure shows the structure of the different traditional techniques.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Recommender Systems" srcset="/posts/2023-01-14-rl-recommendation-systems/recommender_systems_hu_fb75ea082a97f93b.png 330w,
/posts/2023-01-14-rl-recommendation-systems/recommender_systems_hu_cced3cc9cd188591.png 660w,
/posts/2023-01-14-rl-recommendation-systems/recommender_systems_hu_a9df46a2d6e4d929.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/recommender_systems.png src=/posts/2023-01-14-rl-recommendation-systems/recommender_systems.png></figure></p><p>Traditionally, these systems are classified into two main categories: those based on <strong>collaborative filtering</strong> and those based on <strong>content</strong>. However, hybrid techniques that combine elements of both approaches will not be addressed in this article.</p><h4 class="relative group">1.1.1 Collaborative Filtering-Based Recommendation Systems<div id=111-collaborative-filtering-based-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#111-collaborative-filtering-based-recommendation-systems aria-label=Anchor>#</a></span></h4><p>This method builds a user-item interaction matrix, which collects users&rsquo; previous interactions with items. Items are the products that are being recommended to the user (songs, films, books, etc.).</p><p>This matrix is used to identify similar profiles based on their proximity and learn from their interests to make recommendations to users.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Matrix User Element" srcset="/posts/2023-01-14-rl-recommendation-systems/matrix-user-element_hu_ca5dd29b433c9b2e.png 330w,
/posts/2023-01-14-rl-recommendation-systems/matrix-user-element_hu_94e353e9c21c3a0b.png 660w,
/posts/2023-01-14-rl-recommendation-systems/matrix-user-element_hu_d260f9f1adcbea52.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/matrix-user-element.png src=/posts/2023-01-14-rl-recommendation-systems/matrix-user-element.png></figure></p><p>The figure above presents a matrix where each column corresponds to a product to be evaluated, while the rows represent users who have provided ratings for those products. Cells without an assigned value indicate that the user has not yet rated that particular product. The aim, therefore, is to develop a model that predicts the missing interactions in the matrix.</p><p>Within the collaborative filtering approach, two types of methods can be identified: <strong>memory-based</strong>, which use information from similar user/product ratings to make recommendations; and <strong>model-based</strong>, which employ machine learning techniques to create a model that accurately predicts user preferences.</p><h5 class="relative group">Memory-Based Methods<div id=memory-based-methods class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#memory-based-methods aria-label=Anchor>#</a></span></h5><p>In these systems, the previous ratings of a user are used to find a neighbour with similar preferences, then these neighbours’ preferences are combined to generate recommendations.</p><p>There are two techniques: <strong>user-based</strong>, which calculate similarity between users by comparing ratings on the same item, and <strong>item-based</strong>, which make predictions by using the similarity between products.</p><p>To calculate the similarity (or distance) between users or content, various measures can be used, such as Euclidean, Manhattan, or Jaccard distances. The most popular ones are correlation and cosine similarity.</p><h5 class="relative group">Model-Based Methods<div id=model-based-methods class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#model-based-methods aria-label=Anchor>#</a></span></h5><p>These techniques, on the other hand, develop models using machine learning algorithms on the utility matrix to predict users&rsquo; ratings for unrated items.</p><h5 class="relative group">Advantages and Disadvantages of Collaborative Filtering-Based Systems<div id=advantages-and-disadvantages-of-collaborative-filtering-based-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#advantages-and-disadvantages-of-collaborative-filtering-based-systems aria-label=Anchor>#</a></span></h5><p>These systems can recommend products even when there are no user ratings, adapt to changes in user preferences, and provide relevant recommendations without sharing user profile information.</p><p>However, disadvantages include the <strong>cold start</strong> problem, where a recommendation system lacks sufficient information about a user or product to make relevant predictions; <strong>data sparsity</strong>, caused by the lack of rated items in the database; the <strong>scalability</strong> problem, where calculations increase linearly with the number of users and items; and the <strong>synonymy</strong> problem, where systems struggle to distinguish between very similar products.</p><h4 class="relative group">1.1.2 Content-Based Recommendation Systems<div id=112-content-based-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#112-content-based-recommendation-systems aria-label=Anchor>#</a></span></h4><p>This technique does not rely on the user&rsquo;s action history but rather uses information about the product&rsquo;s content to find similarities and recommend similar products to those the user has already shown interest in.</p><p>These models can be based on the <strong>vector space model</strong>, such as Term Frequency-Inverse Document Frequency (TF-IDF), or <strong>probabilistic models</strong>, like Naive Bayes, decision trees, or neural networks.</p><p>Advantages of this technique include independence from other users&rsquo; profile information, as their preferences do not influence recommendations. It also has the ability to quickly adjust recommendations if a user&rsquo;s profile changes, and it can provide explanations for how recommendations were generated.</p><p>The main disadvantage is that a broad knowledge and detailed description of the item&rsquo;s characteristics in the profile are required.</p><h3 class="relative group">1.2 Evaluation Metrics for Recommendation Algorithms<div id=12-evaluation-metrics-for-recommendation-algorithms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#12-evaluation-metrics-for-recommendation-algorithms aria-label=Anchor>#</a></span></h3><p>The quality of a recommendation algorithm can be evaluated using different types of measures, such as accuracy or coverage.</p><p>Accuracy metrics are divided into <strong>statistical accuracy metrics</strong> and <strong>decision-support accuracy metrics</strong>.</p><p><strong>Statistical accuracy metrics</strong> directly compare predicted ratings with actual ratings. One of the most important metrics to evaluate these systems is the Mean Absolute Error (MAE), defined as:</p><p>$$
MAE = \frac{1}{N} \sum_{u,i}^{N} |p_{u,i} - r_{u,i}|
$$</p><p>where \( p_{u,i} \) is the predicted rating for user \( u \) on item \( i \), \( r_{u,i} \) is the actual rating, and \( N \) is the total number of ratings.</p><p><strong>Decision-support accuracy metrics</strong> help users select high-quality items from the available set. Among the most popular are Precision and Recall. The notation <strong>P@K</strong> is used to indicate Precision for a recommendation of <strong>K</strong> objects, and <strong>R@K</strong> for Recall:</p><p>$$
P@K = \frac{|\text{Relevant items in top K} |}{K}
$$</p><p>$$
R@K = \frac{|\text{Relevant items in top K} |}{\text{Relevant items}}
$$</p><p>In addition to metrics, systems can also be evaluated by their coverage, referring to the percentage of items and users for which a recommendation system can provide predictions. Prediction can be almost impossible if no users or very few users rate an item. Coverage may be reduced by defining small neighbourhoods for users or products.</p><h3 class="relative group">1.3 Limitations of Supervised Learning<div id=13-limitations-of-supervised-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#13-limitations-of-supervised-learning aria-label=Anchor>#</a></span></h3><p>These traditional recommendation methods, such as deep neural network-based recommendation systems, are heavily influenced by machine learning techniques, and they present some limitations that we will discuss below.</p><h4 class="relative group">Short-Sighted Recommendation<div id=short-sighted-recommendation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#short-sighted-recommendation aria-label=Anchor>#</a></span></h4><p>Traditional recommendation systems have the issue of providing suggestions that likely lead to immediate responses, often resulting in recommending content a user has previously consumed (short-sighted recommendation).</p><p>This can lead to users being trapped in a &ldquo;recommendation bubble&rdquo;, where they are exposed to an increasingly narrow set of content. In the worst case, it could damage user trust in the long term.</p><h4 class="relative group">System Bias<div id=system-bias class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#system-bias aria-label=Anchor>#</a></span></h4><p>Another problem is that these systems often fail to consider additional factors such as user preferences or system bias, which can result in inaccurate or irrelevant recommendations.</p><h2 class="relative group">2. Recommendation Systems Based on RL<div id=2-recommendation-systems-based-on-rl class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#2-recommendation-systems-based-on-rl aria-label=Anchor>#</a></span></h2><p>Given the limitations of traditional recommendation systems, we can use <strong>reinforcement learning</strong> (RL) to give a new approach to recommending content to users.</p><p>The goal is to capture and discover the dynamic preferences of users to maximize their satisfaction within the platform. This is possible through the <strong>reinforcement learning paradigm</strong>, as it can continuously learn and balance showing both relevant and new content to generate new user interests.</p><p>However, applying this paradigm is not straightforward and comes with several challenges that need to be addressed.</p><h3 class="relative group">2.1 What is Reinforcement Learning?<div id=21-what-is-reinforcement-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#21-what-is-reinforcement-learning aria-label=Anchor>#</a></span></h3><p><strong>Reinforcement learning</strong> (RL) is the technique closest to human learning that can be achieved with current systems. RL consists of modeling the interaction of an agent (like a robot or a machine) within an environment over time to guide its learning process.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="RL Image" srcset="/posts/2023-01-14-rl-recommendation-systems/RL_image_hu_37a5a0d9d6e51781.png 330w,
/posts/2023-01-14-rl-recommendation-systems/RL_image_hu_85fca8aa3f14059b.png 660w,
/posts/2023-01-14-rl-recommendation-systems/RL_image_hu_65f73d0fb539d830.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/RL_image.png src=/posts/2023-01-14-rl-recommendation-systems/RL_image.png></figure></p><p>The agent is placed in an unknown environment, where it must make decisions to reach a specific goal. Through trial and error, the agent learns which actions lead to positive outcomes (which it will repeat) and which actions result in negative outcomes (which it will avoid).</p><p>In these systems, the set of possible states is defined as \( S \), with a particular state \( s \in S \). The set of possible actions is \( A \), with a particular action \( a \in A \), and the set of possible rewards is \( R \), with a particular reward \( r \in R \).</p><p>Reinforcement learning has a wide range of applications, including natural language processing, marketing, and automated robots. However, its application in recommendation systems is particularly interesting.</p><h3 class="relative group">2.2 Elements of Reinforcement Learning<div id=22-elements-of-reinforcement-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#22-elements-of-reinforcement-learning aria-label=Anchor>#</a></span></h3><p>In addition to the agent and environment, it is important to identify other key elements in RL.</p><h4 class="relative group">Policy<div id=policy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#policy aria-label=Anchor>#</a></span></h4><p>A <strong>policy</strong> is a function that determines which action to take in a given state. It can be a deterministic policy, where a single action is taken with probability 1, or a stochastic policy, which defines a probability distribution over multiple actions for each state.</p><p>The policy is the core of RL, as it stores the agent&rsquo;s learning. The goal is to learn an <strong>optimal policy</strong>.</p><h4 class="relative group">Reward Signals<div id=reward-signals class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#reward-signals aria-label=Anchor>#</a></span></h4><p>During iterations in the RL system, the environment sends a numerical value called <strong>reward</strong>, which can be defined as a function of the state $f: S \rightarrow R$. The sole objective of the agent is to maximize the cumulative reward it receives over time, which serves as a quality metric at a specific moment $t$.</p><h4 class="relative group">Value Functions<div id=value-functions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#value-functions aria-label=Anchor>#</a></span></h4><p>While rewards are immediate and assigned to the current state, <strong>value functions</strong> consider long-term desirability. The value function represents the total expected reward an agent can accumulate starting from a particular state.</p><p>From a human perspective, rewards would be analogous to immediate pleasure or pain, while value functions represent a more forward-looking evaluation of our satisfaction with the current state.</p><h3 class="relative group">2.3 Why Use RL in Recommendation Algorithms?<div id=23-why-use-rl-in-recommendation-algorithms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#23-why-use-rl-in-recommendation-algorithms aria-label=Anchor>#</a></span></h3><p>The use of <strong>Reinforcement Learning (RL)</strong> in recommendation systems is justified by the <strong>sequential nature</strong> of the interaction between the user and the system. This problem can be modeled as a Markov process and solved using RL algorithms.</p><p>RL offers three unique advantages not found in traditional recommendation systems:</p><ol><li>Ability to handle the sequential interaction dynamics between the user and the system.</li><li>Consideration of long-term user engagement in decision-making.</li><li>Capability to optimize the recommendation policy without explicit user ratings.</li></ol><h3 class="relative group">2.4 RL Algorithms in Recommendation Systems<div id=24-rl-algorithms-in-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#24-rl-algorithms-in-recommendation-systems aria-label=Anchor>#</a></span></h3><p>The goal of using RL in recommendation systems is to find the <strong>optimal policy</strong> that maximizes the expected cumulative reward over time, providing more accurate and personalized recommendations to users.</p><h4 class="relative group">2.4.1 Deep Q Learning<div id=241-deep-q-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#241-deep-q-learning aria-label=Anchor>#</a></span></h4><p>As previously mentioned, using the classic <strong>Q-Learning</strong> method in real-world problems is computationally infeasible due to the large state and action spaces. In 2015, <strong>DeepMind</strong> introduced <strong>DQN</strong> (Deep Q Network), which marked the beginning of <strong>Deep Reinforcement Learning</strong>.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Deep Q-Learning" srcset="/posts/2023-01-14-rl-recommendation-systems/deepQLearning_hu_e1bb95b00b84f5d.png 330w,
/posts/2023-01-14-rl-recommendation-systems/deepQLearning_hu_f10e6735255ec774.png 660w,
/posts/2023-01-14-rl-recommendation-systems/deepQLearning_hu_7ed097d928db6be8.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/deepQLearning.png src=/posts/2023-01-14-rl-recommendation-systems/deepQLearning.png></figure></p><p>This architecture uses a deep neural network instead of a table to estimate the Q-function, making it more efficient in terms of memory.</p><h5 class="relative group">Deep Q Learning with Experience Replay<div id=deep-q-learning-with-experience-replay class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#deep-q-learning-with-experience-replay aria-label=Anchor>#</a></span></h5><p>DQN includes the <strong>experience replay</strong> technique, which stores information obtained after each interaction in memory and uses it to train the model later. This ensures that the information obtained is better utilized, improving the learning process.</p><h5 class="relative group">Double Deep Q-Learning<div id=double-deep-q-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#double-deep-q-learning aria-label=Anchor>#</a></span></h5><p><strong>Double DQN</strong> addresses the problem of <strong>overestimation</strong> in reinforcement learning. This issue arises when the Q-values overestimate the potential reward. Double DQN solves this problem by using two neural networks: one to select the action and another to evaluate the chosen action.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt=Matrix srcset="/posts/2023-01-14-rl-recommendation-systems/DoubleDQN_hu_c7013a9bce00273c.png 330w,
/posts/2023-01-14-rl-recommendation-systems/DoubleDQN_hu_3c21cec8c3e1123e.png 660w,
/posts/2023-01-14-rl-recommendation-systems/DoubleDQN_hu_8a6541555f8980ee.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/DoubleDQN.png src=/posts/2023-01-14-rl-recommendation-systems/DoubleDQN.png><figcaption>Double DQN</figcaption></figure></p><h5 class="relative group">Dueling Deep Q-Learning<div id=dueling-deep-q-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#dueling-deep-q-learning aria-label=Anchor>#</a></span></h5><p><strong>Dueling DQN</strong> is a variant of the DQN algorithm that proposes a new way to compute Q-values. The neural network is split into two parts: one estimates the <strong>state-value function</strong> \( V(s) \), while the other estimates the <strong>advantage function</strong> \( A(s,a) \).</p><p>The final layer combines these values through a specific aggregation to adjust the final Q-value. This approach improves learning efficiency, though it can lead to &ldquo;unidentifiable&rdquo; problems during training.</p><p>To address this, it forces the highest Q-value to be equal to \( V \), while the highest value in the advantage function is zero, and the rest are negative.</p><h4 class="relative group">2.4.2 Soft Actor-Critic (SAC)<div id=242-soft-actor-critic-sac class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#242-soft-actor-critic-sac aria-label=Anchor>#</a></span></h4><p><strong>Soft Actor-Critic (SAC)</strong> is a model-free, off-policy reinforcement learning algorithm developed by experts from UC Berkeley and Google. It focuses on maximizing both the expected return and the expected entropy of the policy, which allows for more efficient exploration of the action space.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Soft Actor-Critic (SAC)" srcset="/posts/2023-01-14-rl-recommendation-systems/SoftActor-Critic_hu_26a46a5232360914.jpg 330w,
/posts/2023-01-14-rl-recommendation-systems/SoftActor-Critic_hu_59412eda17a6693c.jpg 660w,
/posts/2023-01-14-rl-recommendation-systems/SoftActor-Critic_hu_9b7c7e500d1c465b.jpg 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/SoftActor-Critic.jpg src=/posts/2023-01-14-rl-recommendation-systems/SoftActor-Critic.jpg></figure></p><p>This technique is implemented by parameterizing a Gaussian policy and a Q-function using a neural network and optimizing them using approximate dynamic programming.</p><h4 class="relative group">2.4.3 REINFORCE Top-K Off-Policy Correction<div id=243-reinforce-top-k-off-policy-correction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#243-reinforce-top-k-off-policy-correction aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="REINFORCE Top-K Off-Policy" srcset="/posts/2023-01-14-rl-recommendation-systems/topkoffpolicy_hu_dea589ae812b629a.png 330w,
/posts/2023-01-14-rl-recommendation-systems/topkoffpolicy_hu_ee6e1feb27b185ff.png 660w,
/posts/2023-01-14-rl-recommendation-systems/topkoffpolicy_hu_c77df366d16c0608.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/topkoffpolicy.png src=/posts/2023-01-14-rl-recommendation-systems/topkoffpolicy.png></figure></p><p>The <strong>REINFORCE Top-K Off-Policy Correction</strong> algorithm is based on the REINFORCE algorithm and uses recorded data as a behavior policy to generate a new target policy. However, since the data is generated by the model itself, it may be highly biased, so this algorithm learns from biased data to correct those biases.</p><p>In addition, <strong>Top-K Off-Policy Correction</strong> is introduced, where the agent learns to maximize K rewards instead of just one, selecting the TOP-K.</p><p>In summary, the <strong>REINFORCE Top-K Off-Policy Correction</strong> algorithm uses policy correction and selection of the K most promising actions to improve learning efficiency and stability in an RL environment.</p><h3 class="relative group">2.5 Challenges in Applying Reinforcement Learning to Recommendation Systems<div id=25-challenges-in-applying-reinforcement-learning-to-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#25-challenges-in-applying-reinforcement-learning-to-recommendation-systems aria-label=Anchor>#</a></span></h3><p>While RL addresses several issues with traditional recommendation systems, it introduces new challenges when put into practice.</p><h4 class="relative group">Large Action Space<div id=large-action-space class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#large-action-space aria-label=Anchor>#</a></span></h4><p>This refers to the vast number of possible actions a recommendation system based on RL must consider at any given moment.</p><p>For example, in a system recommending YouTube videos, the action space includes the massive number of videos available on the platform.</p><p>The large action space represents a major challenge, as the larger the action space, the more complex it becomes to learn user preferences and select the most suitable items (e.g., videos) to recommend.</p><h4 class="relative group">Costly Exploration<div id=costly-exploration class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#costly-exploration aria-label=Anchor>#</a></span></h4><p>Exploration can be costly in these systems because it requires testing different actions and adapting to changes in user preferences. However, it is crucial to perform effective exploration because recommending random content could result in a poor user experience.</p><h4 class="relative group">Off-Policy Learning<div id=off-policy-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#off-policy-learning aria-label=Anchor>#</a></span></h4><p><strong>Off-policy learning</strong> allows the system to learn from past experiences different from the current policy, improving its ability to adapt to new situations. However, implementing this technique can be challenging due to the complexity in selecting and processing training data.</p><h4 class="relative group">Partial Observability<div id=partial-observability class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#partial-observability aria-label=Anchor>#</a></span></h4><p>When building a recommendation system, the user does not explicitly inform what they are interested in, so the system must infer user interests from their activities on the platform.</p><h4 class="relative group">Noisy Rewards<div id=noisy-rewards class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#noisy-rewards aria-label=Anchor>#</a></span></h4><p><strong>Noisy rewards</strong> refer to the scattered and unclear feedback from users. This could be due to many reasons, such as lack of contextual information, discomfort in providing feedback, or simply because the user doesn’t know what they want. Finding ways to minimize this noise is crucial.</p><h2 class="relative group">3. Applications<div id=3-applications class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#3-applications aria-label=Anchor>#</a></span></h2><h3 class="relative group">3.1 RecNN: Recommendation Toolkit<div id=31-recnn-recommendation-toolkit class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#31-recnn-recommendation-toolkit aria-label=Anchor>#</a></span></h3><p><a href=https://github.com/awarebayes/RecNN target=_blank>RecNN</a> is a library focused on reinforcement learning for news recommendation systems.</p><p>The main innovation is online learning, meaning there is no need to follow a pre-established policy. Additionally, it uses dynamically generated <em>embeddings</em>, meaning that the representation vectors for items (such as news articles) are created and updated in real-time, allowing for a more accurate and up-to-date representation of the items.</p><h4 class="relative group">3.1.1 Features<div id=311-features class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#311-features aria-label=Anchor>#</a></span></h4><ul><li><p><strong>Control over abstraction level</strong>: You can import a complete algorithm and tell it to train, import networks and their learning functions separately, create a custom loader for your task, or define everything yourself. You can even define your own data.</p></li><li><p>State representation module with various methods.</p></li><li><p><strong>Learning</strong> is based on a sequential or frame environment that supports variable-length data, such as ML20M. The sequence (seq) is a complete sequence of dynamic size (under development), while the frame (frame) is just a static frame.</p></li><li><p><strong>Parallel data loading with Modin (Dask/Ray) and caching</strong>.</p></li><li><p>Supports Pytorch 1.7 with <em>Tensorboard</em> visualization.</p></li></ul><h4 class="relative group">3.1.2 Getting Started<div id=312-getting-started class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#312-getting-started aria-label=Anchor>#</a></span></h4><h5 class="relative group">Import the Library<div id=import-the-library class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#import-the-library aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>recnn</span>
</span></span></code></pre></div><h5 class="relative group">Set Up the Environment<div id=set-up-the-environment class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#set-up-the-environment aria-label=Anchor>#</a></span></h5><p>When working with recommendation environments, you have the option to use static-length inputs or dynamic-length time series with sequential encoders.</p><p>In this case, we will use static length via the <code>FrameEnv</code> class.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>frame_size</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>25</span>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>env</span><span class=o>.</span><span class=n>FrameEnv</span><span class=p>(</span><span class=s2>&#34;ml20_pca128.pkl&#34;</span><span class=p>,</span> <span class=s2>&#34;ml-20m/ratings.csv&#34;</span><span class=p>,</span> <span class=n>frame_size</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span></code></pre></div><h5 class="relative group">Retrieve the Data<div id=retrieve-the-data class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#retrieve-the-data aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>train_batch</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>test</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>train_batch</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>,</span> <span class=n>done</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>get_base_batch</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cpu&#39;</span><span class=p>))</span>
</span></span></code></pre></div><h5 class="relative group">Initialize the Main Networks<div id=initialize-the-main-networks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#initialize-the-main-networks aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>value_net</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Critic</span><span class=p>(</span><span class=mi>1290</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mf>54e-2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>policy_net</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Actor</span><span class=p>(</span><span class=mi>1290</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mf>6e-1</span><span class=p>)</span>
</span></span></code></pre></div><h5 class="relative group">Attempt to Recommend<div id=attempt-to-recommend class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#attempt-to-recommend aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>recommendation</span> <span class=o>=</span> <span class=n>policy_net</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>value</span> <span class=o>=</span> <span class=n>value_net</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>recommendation</span><span class=p>)</span>
</span></span></code></pre></div><h5 class="relative group">Choose to Import a Preferred RL Algorithm<div id=choose-to-import-a-preferred-rl-algorithm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#choose-to-import-a-preferred-rl-algorithm aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ddpg</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>DDPG</span><span class=p>(</span><span class=n>policy_net</span><span class=p>,</span> <span class=n>value_net</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plotter</span> <span class=o>=</span> <span class=n>recnn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>Plotter</span><span class=p>(</span><span class=n>ddpg</span><span class=o>.</span><span class=n>loss_layout</span><span class=p>,</span> <span class=p>[[</span><span class=s1>&#39;value&#39;</span><span class=p>,</span> <span class=s1>&#39;policy&#39;</span><span class=p>]])</span>
</span></span></code></pre></div><h5 class="relative group">Start Training the Algorithm<div id=start-training-the-algorithm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#start-training-the-algorithm aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=n>env</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>ddpg</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>learn</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>plotter</span><span class=o>.</span><span class=n>log_losses</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ddpg</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></div><p>For more information, check out the official documentation <a href=https://recnn.readthedocs.io/en/latest/ target=_blank>here</a>.</p><h3 class="relative group">3.2 RL4RS<div id=32-rl4rs class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#32-rl4rs aria-label=Anchor>#</a></span></h3><p><a href=https://github.com/fuxiAIlab/RL4RS target=_blank>RL4RS</a> is a reinforcement learning-based recommendation system for professionals and researchers.</p><p>This library aims to address common issues in RL-based recommendation systems, such as the gap with reality and lack of validation before deployment.</p><h4 class="relative group">3.2.1 Features<div id=321-features class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#321-features aria-label=Anchor>#</a></span></h4><ul><li><p><strong>Data Understanding Tool</strong>: RL4RS provides a data understanding tool to test the appropriate use of RL on recommendation system datasets.</p></li><li><p>This library is compatible with leading RL libraries such as RLlib and Tianshou, and provides example code for model-free algorithms for both discrete and continuous environments, as well as offline reinforcement learning algorithm implementations.</p></li><li><p><strong>Easy-to-use and Scalable API</strong>: The RL4RS library has a user-friendly and scalable code structure with low coupling to reduce dependencies. It features a file-based gym environment that allows random sampling and sequential access to large datasets, and can be extended to distributed file systems. It also supports Vector Env and is encapsulated in an HTTP interface for deployment across multiple servers, accelerating sample generation.</p></li></ul><h4 class="relative group">3.2.2 Getting Started<div id=322-getting-started class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#322-getting-started aria-label=Anchor>#</a></span></h4><h5 class="relative group">Import the Libraries<div id=import-the-libraries class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#import-the-libraries aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gym</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>rl4rs.env.slate</span> <span class=kn>import</span> <span class=n>SlateRecEnv</span><span class=p>,</span> <span class=n>SlateState</span>
</span></span></code></pre></div><h5 class="relative group">Set Up the Simulation Environment<div id=set-up-the-simulation-environment class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#set-up-the-simulation-environment aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sim</span> <span class=o>=</span> <span class=n>SlateRecEnv</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>state_cls</span><span class=o>=</span><span class=n>SlateState</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;SlateRecEnv-v0&#39;</span><span class=p>,</span> <span class=n>recsim</span><span class=o>=</span><span class=n>sim</span><span class=p>)</span>
</span></span></code></pre></div><h5 class="relative group">Train the Model<div id=train-the-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#train-the-model aria-label=Anchor>#</a></span></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=p>[</span><span class=s2>&#34;max_steps&#34;</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>offline_action</span>
</span></span><span class=line><span class=cl>        <span class=n>next_obs</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>done</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span></code></pre></div><h2 class="relative group">4. Practical Case: RL for YouTube Video Recommendation Systems<div id=4-practical-case-rl-for-youtube-video-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#4-practical-case-rl-for-youtube-video-recommendation-systems aria-label=Anchor>#</a></span></h2><p>In this section, we will analyze a real-world case of implementing a reinforcement learning-based recommendation system. In this case, it will be YouTube video recommendation, developed by Google.</p><h3 class="relative group">4.1 Introduction<div id=41-introduction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#41-introduction aria-label=Anchor>#</a></span></h3><p>Access to online content has become a necessity, with YouTube being one of the most popular platforms where millions of people consume videos every day. However, traditional systems might lead to users getting trapped in a &ldquo;bubble&rdquo; or receiving irrelevant recommendations, causing a loss of trust. The goal is to adapt to and discover users&rsquo; dynamic preferences to optimize long-term utility, which can be achieved through reinforcement learning.</p><h3 class="relative group">4.2 Candidate Generation<div id=42-candidate-generation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#42-candidate-generation aria-label=Anchor>#</a></span></h3><p>The system that produces YouTube video recommendations is a multi-stage recommender that selects dozens of videos for users from a corpus consisting of billions of videos.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Candidate Generator" srcset="/posts/2023-01-14-rl-recommendation-systems/candidate_generator_hu_f41408ba6883a06d.png 330w,
/posts/2023-01-14-rl-recommendation-systems/candidate_generator_hu_7fed04713b43bf6f.png 660w,
/posts/2023-01-14-rl-recommendation-systems/candidate_generator_hu_491329feba823080.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/candidate_generator.png src=/posts/2023-01-14-rl-recommendation-systems/candidate_generator.png></figure></p><p>This study focuses only on the <strong>candidate generation</strong> stage, where the video corpus is narrowed down to a few hundred more relevant videos to move on to the next stage.</p><p>Several challenges have been encountered in building this structure, as the recommendation system must handle billions of users with changing preferences over time, billions of videos with few views but relevant to a small group of users (cold-start problem), and noisy and sparse user feedback.</p><h3 class="relative group">4.3 Reinforcement Learning in Recommendation Systems<div id=43-reinforcement-learning-in-recommendation-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#43-reinforcement-learning-in-recommendation-systems aria-label=Anchor>#</a></span></h3><p>Once we understand the concept of candidate generation, we will base it on reinforcement learning.</p><p>The goal is to build agents that perform actions in an environment to maximize a notion of cumulative reward. In this context, our <strong>agent</strong> is the candidate generator, the <strong>states</strong> are user interests and recommendation contacts, the <strong>reward</strong> is user satisfaction, and the <strong>actions</strong> the agent can take involve selecting and proposing videos to be included in a catalog of millions of videos.</p><h3 class="relative group">4.4 Model Construction<div id=44-model-construction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#44-model-construction aria-label=Anchor>#</a></span></h3><p>In this section, we will see how YouTube developers have constructed the video recommendation agent using reinforcement learning.</p><h4 class="relative group">Agent and Reward<div id=agent-and-reward class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#agent-and-reward aria-label=Anchor>#</a></span></h4><p>The data source used to build the agent is user trajectory, i.e., a sequence of activities performed by the user on the platform (such as watched videos, searches, etc.).</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Agent Reward" srcset="/posts/2023-01-14-rl-recommendation-systems/agent_reward_hu_635ee9af12565196.png 330w,
/posts/2023-01-14-rl-recommendation-systems/agent_reward_hu_c143b310dce68a42.png 660w,
/posts/2023-01-14-rl-recommendation-systems/agent_reward_hu_4b2fd253c9569739.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/agent_reward.png src=/posts/2023-01-14-rl-recommendation-systems/agent_reward.png></figure></p><p>This information is divided into <strong>past sequential trajectory</strong>, composed of activities before the agent&rsquo;s recommendations, and <strong>future sequential trajectory</strong>, containing information about activities the user has performed after receiving the agent&rsquo;s recommendations.</p><p>The presentation author mentions using the past sequential trajectory to arrive at the &ldquo;user state belief&rdquo; and using the <strong>future trajectory</strong> to derive the reward.</p><h4 class="relative group">States<div id=states class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#states aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="User State Representation via RNN" srcset="/posts/2023-01-14-rl-recommendation-systems/states_hu_b40ec2d2a0665e51.png 330w,
/posts/2023-01-14-rl-recommendation-systems/states_hu_8c34b866c5a15abf.png 660w,
/posts/2023-01-14-rl-recommendation-systems/states_hu_172c92cb1cf5add.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/states.png src=/posts/2023-01-14-rl-recommendation-systems/states.png></figure></p><p>One of the major challenges in building state representation is partial observability because users do not provide information about their interests or how satisfied they are with the recommendations given. To address this challenge, recurrent neural networks (RNNs) are used to analyze prior user activity and obtain a state representation.</p><h4 class="relative group">Actions<div id=actions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#actions aria-label=Anchor>#</a></span></h4><p>In this case, a policy-based approach is used because we want to directly maximize long-term reward, which is more stable than a value-based approach.</p><p>We are trying to learn a stochastic policy (Pi theta) that will issue a distribution over the action-state space, aiming to maximize long-term accumulated reward.</p><p><figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt="Policy RL" srcset="/posts/2023-01-14-rl-recommendation-systems/policyRL_hu_bc5cd0d488a549f2.png 330w,
/posts/2023-01-14-rl-recommendation-systems/policyRL_hu_91d4416bc9bc7742.png 660w,
/posts/2023-01-14-rl-recommendation-systems/policyRL_hu_7237a46f21d3d47b.png 1280w" data-zoom-src=/posts/2023-01-14-rl-recommendation-systems/policyRL.png src=/posts/2023-01-14-rl-recommendation-systems/policyRL.png></figure></p><p>The trajectory $\tau$ is generated following this policy, and the accumulated reward is the total sum of rewards over the entire trajectory. Additionally, policy parameters can be optimized through gradient ascent.</p><p>As a result, it can be said that reinforcement learning is closely related to supervised learning because it is optimized for the likelihood of observing the next action. However, the reinforcement learning paradigm offers a way to think about exploration, planning, and changes in the underlying user state.</p><p>A sampling technique is used to handle the very large action space and perform quick neighborhood searches to reduce processing time.</p><h3 class="relative group">4.5 Addressing Machine Learning Limitations<div id=45-addressing-machine-learning-limitations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#45-addressing-machine-learning-limitations aria-label=Anchor>#</a></span></h3><p>In this part, we will see how they have addressed the two limitations of traditional recommendation systems.</p><h4 class="relative group">Myopic Recommendation<div id=myopic-recommendation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#myopic-recommendation aria-label=Anchor>#</a></span></h4><p>To tackle the problem of myopic recommendation, the solution is to incorporate future rewards rather than only considering immediate rewards.</p><p>In experiments, incorporating these future rewards led to a 0.3% gain in the online matrix.</p><h4 class="relative group">System Bias<div id=system-bias-1 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#system-bias-1 aria-label=Anchor>#</a></span></h4><p>This system only has access to log data generated by an agent updated every 5 hours, which means the agent&rsquo;s policy may be very different from the target policy being learned. Therefore, the team continues to study how to handle system bias caused by having access only to these log data.</p><h2 class="relative group">5. Conclusions<div id=5-conclusions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#5-conclusions aria-label=Anchor>#</a></span></h2><p>In conclusion, reinforcement learning in recommendation systems offers unique features that allow for better suggestions compared to traditional recommendation systems.</p><p>On the other hand, within reinforcement learning-based recommendation systems, traditional algorithms like policy iteration or Q-Learning do not adequately meet expectations in the recommendation domain.</p><p>Similarly, there are specialized libraries for creating reinforcement learning-based recommendation environments.</p><p>Additionally, it is noteworthy that many digital platforms like YouTube use reinforcement learning to improve the accuracy of their recommendation algorithms.</p><h2 class="relative group">6. References<div id=6-references class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#6-references aria-label=Anchor>#</a></span></h2><ul><li><p>F.O. Isinkaye, Y.O. Folajimi, & B.A. Ojokoh (2015). Recommendation systems: Principles, methods and evaluation. Egyptian Informatics Journal, 16(3), 261-273.</p></li><li><p>Srinivasan, A. (2022, January 31). Recommendation Systems using Reinforcement Learning. Medium. <a href=https://medium.com/ibm-data-ai/recommendation-systems-using-reinforcement-learning-de6379eecfde target=_blank>https://medium.com/ibm-data-ai/recommendation-systems-using-reinforcement-learning-de6379eecfde</a></p></li><li><p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. ArXiv: Learning. <a href=http://cs.nyu.edu/~koray/publis/mnih-atari-2013.pdf target=_blank>http://cs.nyu.edu/~koray/publis/mnih-atari-2013.pdf</a></p></li><li><p>Hasselt, H., Guez, A., & Silver, D.. (2015). Deep Reinforcement Learning with Double Q-learning.</p></li><li><p>Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N.. (2015). Dueling Network Architectures for Deep Reinforcement Learning.</p></li><li><p>M Scherbina. (2019). RecNN: RL Recommendation with PyTorch.</p></li><li><p>Kai Wang, Zhene Zou, Yue Shang, Qilin Deng, Minghao Zhao, Runze Wu, Xudong Shen, Tangjie Lyu, & Changjie Fan (2021). RL4RS: A Real-World Benchmark for Reinforcement Learning based Recommender System. ArXiv, abs/2110.11073.</p></li><li><p>Association for Computing Machinery (ACM). (2019, March 28). “Reinforcement Learning for Recommender Systems: A Case Study on Youtube,” by Minmin Chen <a href>Video</a></p></li></ul></div></div><script type=text/javascript src=/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA==" data-oid=views_posts/2023-01-14-rl-recommendation-systems/index.md data-oid-likes=likes_posts/2023-01-14-rl-recommendation-systems/index.md></script></section><footer class="pt-8 max-w-prose print:hidden"></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/categories/ title=Categories>Categories</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Kenny Flores</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500" data-url=https://kennyfh.github.io/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>